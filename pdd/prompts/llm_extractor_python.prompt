<pdd-reason>Handles semantic extraction from files using LLMs with persistent caching for reproducibility.</pdd-reason>

<pdd-interface>
{
  "type": "module",
  "module": {
    "functions": [
      {"name": "LLMExtractor.extract", "signature": "(file_path: str, query: str, force_refresh: bool = False)", "returns": "str"}
    ]
  }
}
</pdd-interface>

% You are an expert Python engineer. Your goal is to create a module for semantic extraction from files using LLMs.

% Role & Scope
The `llm_extractor` module handles fuzzy or semantic queries against large documents that cannot be handled by structural selectors. It implements a persistent caching mechanism to ensure reproducibility and token efficiency.

% Requirements
1. Implement `LLMExtractor` class with an `extract(file_path: str, query: str, force_refresh: bool = False) -> str` method.
2. Use `llm_invoke` (from `pdd.llm_invoke`) to perform semantic extraction based on the provided query.
3. Implement persistent caching in `.pdd/extracts/`.
4. Cache structure:
   - Content file: `.pdd/extracts/<cache_key>.md`
   - Metadata file: `.pdd/extracts/<cache_key>.meta.json` containing: `source_path`, `source_hash`, `query`, `timestamp`, `token_count`.
5. Generate deterministic `cache_key` based on `source_path` and `query`.
6. Staleness detection:
   - Before using cache, compare current source file hash with `source_hash` in metadata.
   - If stale, warn the user but use the cache unless `force_refresh` is True.
7. Support `force_refresh` to bypass cache and re-run LLM extraction.
8. Follow the "Lockfile pattern": extracts are intended to be committed to version control for reproducible builds.
9. Use `rich` for status updates (e.g., "Extracting...", "Using cache...").

% Dependencies
<llm_invoke>
  <include>pdd/llm_invoke.py</include>
</llm_invoke>

% Deliverables
- Code: `pdd/llm_extractor.py`
